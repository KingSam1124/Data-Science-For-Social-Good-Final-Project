print(paste("Scraping topic:", topic))
# Get the titles for the topic
page_titles <- search_wikipedia(topic, num_results)
# For each title, scrape the first paragraph
for (title in page_titles) {
paragraph <- get_first_paragraph(title)
all_data <- rbind(all_data, data.frame(topic = topic, title = title, first_paragraph = paragraph, stringsAsFactors = FALSE))
}
}
return(all_data)
}
# List of topics to scrape
topics <- c("History", "Sports", "Technology", "Politics")
# Scrape the data
scraped_data <- scrape_wikipedia_data(topics)
# Function to get the page titles for a given topic
search_wikipedia <- function(topic, num_results = 50) {
# Define Wikipedia search URL
search_url <- paste0("https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch=",
URLencode(topic), "&format=json&srprop=snippet&utf8=")
# Get the search results
response <- GET(search_url)
# Parse the response content as raw text (no need for "parsed" argument)
content_data <- content(response, "text")  # Get the content as text
# Parse the JSON content using jsonlite
content_parsed <- fromJSON(content_data)
# Extract page titles from the response
search_results <- content_parsed$query$search
page_titles <- sapply(search_results, function(x) x$title)
return(page_titles[1:num_results])  # Return the top `num_results` titles
}
# Function to get the first non-empty paragraph of a Wikipedia page given the title
get_first_paragraph <- function(title) {
page_url <- paste0("https://en.wikipedia.org/wiki/", URLencode(title))
# Scrape the page content
page <- tryCatch({
read_html(page_url)
}, error = function(e) {
NULL  # If an error occurs, return NULL
})
# Extract the first paragraph (ignoring unwanted elements)
if (!is.null(page)) {
paragraphs <- page %>%
html_nodes("p") %>%
html_text()
# Clean the paragraphs and filter out empty or irrelevant content
paragraphs <- paragraphs[!str_detect(paragraphs, "^\\s*$")]  # Remove empty paragraphs
# If there is at least one valid paragraph, return the first one
if (length(paragraphs) > 0) {
return(paragraphs[1])
}
}
return(NA)  # Return NA if no valid paragraph is found
}
# Main function to scrape Wikipedia data for multiple topics
scrape_wikipedia_data <- function(topics, num_results = 50) {
all_data <- data.frame(topic = character(), title = character(), first_paragraph = character(), stringsAsFactors = FALSE)
# Loop over each topic to collect data
for (topic in topics) {
print(paste("Scraping topic:", topic))
# Get the titles for the topic
page_titles <- search_wikipedia(topic, num_results)
# For each title, scrape the first paragraph
for (title in page_titles) {
paragraph <- get_first_paragraph(title)
all_data <- rbind(all_data, data.frame(topic = topic, title = title, first_paragraph = paragraph, stringsAsFactors = FALSE))
}
}
return(all_data)
}
# List of topics to scrape
topics <- c("History", "Sports", "Technology", "Politics")
# Scrape the data
scraped_data <- scrape_wikipedia_data(topics)
# Function to get the page titles for a given topic
search_wikipedia <- function(topic, num_results = 50) {
# Define Wikipedia search URL
search_url <- paste0("https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch=",
URLencode(topic), "&format=json&srprop=snippet&utf8=")
# Get the search results
response <- GET(search_url)
content <- content(response, "parsed")
# Extract page titles from the response
search_results <- content$query$search
page_titles <- sapply(search_results, function(x) x$title)
return(page_titles[1:num_results])  # Return the top `num_results` titles
}
# Function to get the first non-empty paragraph of a Wikipedia page given the title
get_first_paragraph <- function(title) {
page_url <- paste0("https://en.wikipedia.org/wiki/", URLencode(title))
# Scrape the page content
page <- tryCatch({
read_html(page_url)
}, error = function(e) {
NULL  # If an error occurs, return NULL
})
# Extract the first paragraph (ignoring unwanted elements)
if (!is.null(page)) {
paragraphs <- page %>%
html_nodes("p") %>%
html_text()
# Clean the paragraphs and filter out empty or irrelevant content
paragraphs <- paragraphs[!str_detect(paragraphs, "^\\s*$")]  # Remove empty paragraphs
# If there is at least one valid paragraph, return the first one
if (length(paragraphs) > 0) {
return(paragraphs[1])
}
}
return(NA)  # Return NA if no valid paragraph is found
}
# Function to scrape titles and first paragraphs for multiple topics
scrape_wikipedia_data <- function(topics, num_results = 50) {
# Initialize a list to store all the data
all_data <- data.frame(topic = character(), title = character(), first_paragraph = character(), stringsAsFactors = FALSE)
# Loop over each topic to collect data
for (topic in topics) {
print(paste("Scraping topic:", topic))
# Get the titles for the topic
page_titles <- search_wikipedia(topic, num_results)
# For each title, scrape the first paragraph
for (title in page_titles) {
paragraph <- get_first_paragraph(title)
all_data <- rbind(all_data, data.frame(topic = topic, title = title, first_paragraph = paragraph, stringsAsFactors = FALSE))
}
}
return(all_data)
}
# Choose topics for scraping
topics <- c("History", "Sports", "Technology", "Politics")
# Scrape the data
scraped_data <- scrape_wikipedia_data(topics)
search_wikipedia <- function(topic, num_results = 50) {
# Define Wikipedia search URL
search_url <- paste0("https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch=",
URLencode(topic), "&format=json&srprop=snippet&utf8=")
# Get the search results
response <- GET(search_url)
# Parse the response content as raw text and convert to JSON
content_data <- content(response, "text")  # Get the raw content as text
content_parsed <- fromJSON(content_data)  # Parse the JSON content using jsonlite
# Extract page titles from the response
search_results <- content_parsed$query$search
page_titles <- sapply(search_results, function(x) x$title)
return(page_titles[1:num_results])  # Return the top `num_results` titles
}
# Function to get the first non-empty paragraph of a Wikipedia page given the title
get_first_paragraph <- function(title) {
page_url <- paste0("https://en.wikipedia.org/wiki/", URLencode(title))
# Scrape the page content
page <- tryCatch({
read_html(page_url)
}, error = function(e) {
NULL  # If an error occurs, return NULL
})
# Extract the first paragraph (ignoring unwanted elements)
if (!is.null(page)) {
paragraphs <- page %>%
html_nodes("p") %>%
html_text()
# Clean the paragraphs and filter out empty or irrelevant content
paragraphs <- paragraphs[!str_detect(paragraphs, "^\\s*$")]  # Remove empty paragraphs
# If there is at least one valid paragraph, return the first one
if (length(paragraphs) > 0) {
return(paragraphs[1])
}
}
return(NA)  # Return NA if no valid paragraph is found
}
# Function to scrape titles and first paragraphs for multiple topics
scrape_wikipedia_data <- function(topics, num_results = 50) {
# Initialize a list to store all the data
all_data <- data.frame(topic = character(), title = character(), first_paragraph = character(), stringsAsFactors = FALSE)
# Loop over each topic to collect data
for (topic in topics) {
print(paste("Scraping topic:", topic))
# Get the titles for the topic
page_titles <- search_wikipedia(topic, num_results)
# For each title, scrape the first paragraph
for (title in page_titles) {
paragraph <- get_first_paragraph(title)
all_data <- rbind(all_data, data.frame(topic = topic, title = title, first_paragraph = paragraph, stringsAsFactors = FALSE))
}
}
return(all_data)
}
# Choose topics for scraping
topics <- c("History", "Sports", "Technology", "Politics")
# Scrape the data
scraped_data <- scrape_wikipedia_data(topics)
# Load necessary libraries
library(rvest)
library(httr)
library(dplyr)
library(stringr)
library(jsonlite)
# Function to get the page titles for a given topic
search_wikipedia <- function(topic, num_results = 50) {
# Define Wikipedia search URL
search_url <- paste0("https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch=",
URLencode(topic), "&format=json&srprop=snippet&utf8=")
# Get the search results
response <- GET(search_url)
# Use rawToChar to extract the content from the response
content_data <- rawToChar(response$content)  # Extract the raw content
content_parsed <- fromJSON(content_data)  # Parse the JSON content using jsonlite
# Extract page titles from the response
search_results <- content_parsed$query$search
page_titles <- sapply(search_results, function(x) x$title)
return(page_titles[1:num_results])  # Return the top `num_results` titles
}
# Function to get the first non-empty paragraph of a Wikipedia page given the title
get_first_paragraph <- function(title) {
page_url <- paste0("https://en.wikipedia.org/wiki/", URLencode(title))
# Scrape the page content
page <- tryCatch({
read_html(page_url)
}, error = function(e) {
NULL  # If an error occurs, return NULL
})
# Extract the first paragraph (ignoring unwanted elements)
if (!is.null(page)) {
paragraphs <- page %>%
html_nodes("p") %>%
html_text()
# Clean the paragraphs and filter out empty or irrelevant content
paragraphs <- paragraphs[!str_detect(paragraphs, "^\\s*$")]  # Remove empty paragraphs
# If there is at least one valid paragraph, return the first one
if (length(paragraphs) > 0) {
return(paragraphs[1])
}
}
return(NA)  # Return NA if no valid paragraph is found
}
# Function to scrape titles and first paragraphs for multiple topics
scrape_wikipedia_data <- function(topics, num_results = 50) {
# Initialize a list to store all the data
all_data <- data.frame(topic = character(), title = character(), first_paragraph = character(), stringsAsFactors = FALSE)
# Loop over each topic to collect data
for (topic in topics) {
print(paste("Scraping topic:", topic))
# Get the titles for the topic
page_titles <- search_wikipedia(topic, num_results)
# For each title, scrape the first paragraph
for (title in page_titles) {
paragraph <- get_first_paragraph(title)
all_data <- rbind(all_data, data.frame(topic = topic, title = title, first_paragraph = paragraph, stringsAsFactors = FALSE))
}
}
return(all_data)
}
# Choose topics for scraping
topics <- c("History", "Sports", "Technology", "Politics")
# Scrape the data
scraped_data <- scrape_wikipedia_data(topics)
search_wikipedia <- function(topic, num_results = 50) {
# Define Wikipedia search URL
search_url <- paste0("https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch=",
URLencode(topic), "&format=json&srprop=snippet&utf8=")
# Get the search results
response <- GET(search_url)
# Use rawToChar to extract the content from the response
content_data <- rawToChar(response$content)  # Extract the raw content
content_parsed <- fromJSON(content_data)  # Parse the JSON content using jsonlite
# Print the structure of the parsed JSON to understand its structure
print(str(content_parsed))
# Extract page titles from the response
search_results <- content_parsed$query$search
# Check if search_results is a list and contains titles
if (is.list(search_results)) {
page_titles <- sapply(search_results, function(x) x$title)
} else {
page_titles <- character(0)  # Return an empty vector if no titles are found
}
return(page_titles[1:num_results])  # Return the top `num_results` titles
}
# Function to get the first non-empty paragraph of a Wikipedia page given the title
get_first_paragraph <- function(title) {
page_url <- paste0("https://en.wikipedia.org/wiki/", URLencode(title))
# Scrape the page content
page <- tryCatch({
read_html(page_url)
}, error = function(e) {
NULL  # If an error occurs, return NULL
})
# Extract the first paragraph (ignoring unwanted elements)
if (!is.null(page)) {
paragraphs <- page %>%
html_nodes("p") %>%
html_text()
# Clean the paragraphs and filter out empty or irrelevant content
paragraphs <- paragraphs[!str_detect(paragraphs, "^\\s*$")]  # Remove empty paragraphs
# If there is at least one valid paragraph, return the first one
if (length(paragraphs) > 0) {
return(paragraphs[1])
}
}
return(NA)  # Return NA if no valid paragraph is found
}
# Function to scrape titles and first paragraphs for multiple topics
scrape_wikipedia_data <- function(topics, num_results = 50) {
# Initialize a list to store all the data
all_data <- data.frame(topic = character(), title = character(), first_paragraph = character(), stringsAsFactors = FALSE)
# Loop over each topic to collect data
for (topic in topics) {
print(paste("Scraping topic:", topic))
# Get the titles for the topic
page_titles <- search_wikipedia(topic, num_results)
# For each title, scrape the first paragraph
for (title in page_titles) {
paragraph <- get_first_paragraph(title)
all_data <- rbind(all_data, data.frame(topic = topic, title = title, first_paragraph = paragraph, stringsAsFactors = FALSE))
}
}
return(all_data)
}
# Choose topics for scraping
topics <- c("History", "Sports", "Technology", "Politics")
# Scrape the data
scraped_data <- scrape_wikipedia_data(topics)
# Define the category URLs for the four topics
categories <- list(
Sports = "https://en.wikipedia.org/wiki/Category:Sports",
Technology = "https://en.wikipedia.org/wiki/Category:Technology",
History = "https://en.wikipedia.org/wiki/Category:History",
Politics = "https://en.wikipedia.org/wiki/Category:Politics"
)
# Define a function to extract article URLs from a category page
get_article_urls <- function(category_url, num_articles = 50) {
# Read the category page
category_page <- read_html(category_url)
# Extract links to articles within the category (looking for links with class "mw-category-group a")
article_links <- category_page %>%
html_nodes(".mw-category-group a") %>%
html_attr("href")
# Construct the full URLs for the articles
full_urls <- paste0("https://en.wikipedia.org", article_links)
# Get the first 'num_articles' URLs (ensure to return only unique URLs)
return(unique(full_urls)[1:num_articles])
}
# Define a function to extract the title and first paragraph from a Wikipedia page
get_wiki_data <- function(url) {
# Read the webpage
page <- read_html(url)
# Extract the title (h1)
title <- page %>%
html_nodes("h1") %>%
html_text()
# Extract the first paragraph (p)
paragraph <- page %>%
html_nodes("p") %>%
html_text() %>%
.[1]  # Take the first paragraph
return(data.frame(title = title, first_paragraph = paragraph, stringsAsFactors = FALSE))
}
# Initialize an empty data frame to store the results
all_data <- data.frame(title = character(), first_paragraph = character(), topic = character(), stringsAsFactors = FALSE)
# Loop through each topic (Sports, Technology, History, Politics)
for (topic in names(categories)) {
category_url <- categories[[topic]]
# Get the URLs of the first 50 articles in this category
article_urls <- get_article_urls(category_url)
# Scrape data from each article
for (url in article_urls) {
data <- get_wiki_data(url)
data$topic <- topic  # Add the topic label to the data
all_data <- rbind(all_data, data)
}
}
# Define a function to extract article URLs from a category page
get_article_urls <- function(category_url, num_articles = 50, retries = 3) {
# Retry mechanism in case of failure
for (i in 1:retries) {
tryCatch({
# Read the category page
category_page <- read_html(category_url)
# Extract links to articles within the category
article_links <- category_page %>%
html_nodes(".mw-category-group a") %>%
html_attr("href")
# Construct the full URLs for the articles
full_urls <- paste0("https://en.wikipedia.org", article_links)
# Get the first 'num_articles' URLs
return(unique(full_urls)[1:num_articles])
}, error = function(e) {
# Print error message and retry if possible
message("Error in connection: ", e$message, " - Retrying (", i, "/", retries, ")")
Sys.sleep(2)  # Wait for 2 seconds before retrying
})
}
# If all retries fail, return an empty vector
message("Failed to retrieve data after ", retries, " attempts.")
return(character(0))  # Return an empty vector if all retries fail
}
# Define a function to extract the title and first paragraph from a Wikipedia page
get_wiki_data <- function(url, retries = 3) {
# Retry mechanism in case of failure
for (i in 1:retries) {
tryCatch({
# Read the webpage
page <- read_html(url)
# Extract the title (h1)
title <- page %>%
html_nodes("h1") %>%
html_text()
# Extract the first paragraph (p)
paragraph <- page %>%
html_nodes("p") %>%
html_text() %>%
.[1]  # Take the first paragraph
return(data.frame(title = title, first_paragraph = paragraph, stringsAsFactors = FALSE))
}, error = function(e) {
# Print error message and retry if possible
message("Error in connection: ", e$message, " - Retrying (", i, "/", retries, ")")
Sys.sleep(2)  # Wait for 2 seconds before retrying
})
}
# If all retries fail, return NULL
message("Failed to retrieve data after ", retries, " attempts.")
return(NULL)  # Return NULL if all retries fail
}
# Initialize an empty data frame to store the results
all_data <- data.frame(title = character(), first_paragraph = character(), topic = character(), stringsAsFactors = FALSE)
# Loop through each topic (Sports, Technology, History, Politics)
for (topic in names(categories)) {
category_url <- categories[[topic]]
# Get the URLs of the first 50 articles in this category
article_urls <- get_article_urls(category_url)
# Scrape data from each article
for (url in article_urls) {
data <- get_wiki_data(url)
if (!is.null(data)) {
data$topic <- topic  # Add the topic label to the data
all_data <- rbind(all_data, data)
}
}
}
# Define the categories and their corresponding Wikipedia URLs
categories <- list(
Sports = "https://en.wikipedia.org/wiki/Category:Sports",
Technology = "https://en.wikipedia.org/wiki/Category:Technology",
History = "https://en.wikipedia.org/wiki/Category:History",
Politics = "https://en.wikipedia.org/wiki/Category:Politics"
)
# Function to extract article URLs from a category page
get_article_urls <- function(category_url, num_articles = 50, retries = 3) {
# Retry mechanism in case of failure
for (i in 1:retries) {
tryCatch({
# Read the category page
category_page <- read_html(category_url)
# Extract links to articles within the category
article_links <- category_page %>%
html_nodes(".mw-category-group a") %>%
html_attr("href")
# Construct the full URLs for the articles
full_urls <- paste0("https://en.wikipedia.org", article_links)
# Return the first 'num_articles' URLs
return(unique(full_urls)[1:num_articles])
}, error = function(e) {
# Print error message and retry if possible
message("Error in connection: ", e$message, " - Retrying (", i, "/", retries, ")")
Sys.sleep(2)  # Wait for 2 seconds before retrying
})
}
# If all retries fail, return an empty vector
message("Failed to retrieve data after ", retries, " attempts.")
return(character(0))  # Return an empty vector if all retries fail
}
# Function to extract the title and first paragraph from a Wikipedia page
get_wiki_data <- function(url, retries = 3) {
# Retry mechanism in case of failure
for (i in 1:retries) {
tryCatch({
# Read the webpage
page <- read_html(url)
# Extract the title (h1)
title <- page %>%
html_nodes("h1") %>%
html_text()
# Extract the first paragraph (p)
paragraph <- page %>%
html_nodes("p") %>%
html_text() %>%
.[1]  # Take the first paragraph
return(data.frame(title = title, first_paragraph = paragraph, stringsAsFactors = FALSE))
}, error = function(e) {
# Print error message and retry if possible
message("Error in connection: ", e$message, " - Retrying (", i, "/", retries, ")")
Sys.sleep(2)  # Wait for 2 seconds before retrying
})
}
# If all retries fail, return NULL
message("Failed to retrieve data after ", retries, " attempts.")
return(NULL)  # Return NULL if all retries fail
}
# Initialize an empty data frame to store the results
all_data <- data.frame(title = character(), first_paragraph = character(), topic = character(), stringsAsFactors = FALSE)
# Loop through each topic (Sports, Technology, History, Politics)
for (topic in names(categories)) {
category_url <- categories[[topic]]
# Get the URLs of the first 50 articles in this category
article_urls <- get_article_urls(category_url)
# Scrape data from each article
for (url in article_urls) {
data <- get_wiki_data(url)
if (!is.null(data)) {
data$topic <- topic  # Add the topic label to the data
all_data <- rbind(all_data, data)
}
Sys.sleep(3)  # Wait for 3 seconds between requests to avoid rate limiting
}
}
